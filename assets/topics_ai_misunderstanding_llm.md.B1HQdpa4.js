import{_ as a,c as i,o as t,am as n}from"./chunks/framework.kaNjPv1t.js";const d=JSON.parse('{"title":"长期以来我对 LLM 的误解","description":"深入探讨大语言模型（LLM）的工作原理，理解从传统规则编程到概率计算的范式转换，揭秘模型参数与智能的本质。","frontmatter":{"title":"长期以来我对 LLM 的误解","description":"深入探讨大语言模型（LLM）的工作原理，理解从传统规则编程到概率计算的范式转换，揭秘模型参数与智能的本质。","author":"mcell","tags":["大语言模型","LLM","人工智能","机器学习","深度学习","Transformer","模型参数"],"keywords":["LLM 工作原理","大语言模型理解","Transformer 模型","模型参数","人工智能科普","机器学习基础","概率计算","智能本质"],"head":[["link",{"rel":"canonical","href":"https://stack.mcell.toptopics/ai/misunderstanding_llm"}],["meta",{"property":"og:url","content":"https://stack.mcell.toptopics/ai/misunderstanding_llm"}]]},"headers":[],"relativePath":"topics/ai/misunderstanding_llm.md","filePath":"topics/ai/misunderstanding_llm.md","lastUpdated":1762106888000}'),e={name:"topics/ai/misunderstanding_llm.md"};function l(p,s,r,o,h,g){return t(),i("div",null,s[0]||(s[0]=[n(`<p><img src="https://stack-mcell.tos-cn-shanghai.volces.com/051.png" alt="051.png" loading="lazy"></p><h1 id="长期以来我对-llm-的误解" tabindex="-1">长期以来我对 LLM 的误解 <a class="header-anchor" href="#长期以来我对-llm-的误解" aria-label="Permalink to &quot;长期以来我对 LLM 的误解&quot;">​</a></h1><p>大家好，我是 mCell。</p><p>最近半年，我一直在折腾各种大语言模型（LLM），从 GPT 到 Gemini，再到国产的 DeepSeek、Qwen。我用它们当搜索引擎、写代码、甚至帮我完成期末作业的实验报告。作为一个开发者，我一直在思考一个问题：这东西的底层到底是怎么工作的？</p><h2 id="一个朴素的疑问" tabindex="-1">一个朴素的疑问 <a class="header-anchor" href="#一个朴素的疑问" aria-label="Permalink to &quot;一个朴素的疑问&quot;">​</a></h2><p>我的疑问很简单：<strong>LLM 说到底只是一堆代码，它怎么就能“理解”我说的话呢？</strong></p><p><img src="https://stack-mcell.tos-cn-shanghai.volces.com/053.png" alt="053.png" loading="lazy"></p><p>按照我们传统程序员的思维，一个程序要实现特定功能，就需要明确的逻辑映射。比如，我们要写一个智能客服，代码可能是这样的：</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> handle_query</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(query):</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;天气&quot;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> query </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">and</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;今天&quot;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> query:</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">        return</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> get_todays_weather()</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    elif</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;你好&quot;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> query </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">or</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;早上好&quot;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> query:</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">        return</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;你好！有什么可以帮你的吗？&quot;</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    else</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">        return</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;抱歉，我听不懂你在说什么。&quot;</span></span></code></pre></div><p>这种基于关键词和规则的方式，简单直接，但死板得像个石板。我们必须为每一种可能性预设好程序的分支。很多年前的手机助手，大抵就是这个逻辑。</p><p>但是，现在的 LLM 完全不同。我可以直接对它说“早上好”，它会像真人一样回复“早上好！今天又是元气满满的一天呢”，甚至可能还会根据上下文，附上一句“今天有什么安排吗？”。</p><p>我查看了那些开源模型的代码，并没有找到类似 <code>if query == &quot;早上好&quot;</code> 这样的特殊处理。谁说计算机没有黑魔法，这不就是魔法吗？</p><h2 id="ollama、7gb-文件和代码仓库" tabindex="-1">Ollama、7GB 文件和代码仓库 <a class="header-anchor" href="#ollama、7gb-文件和代码仓库" aria-label="Permalink to &quot;Ollama、7GB 文件和代码仓库&quot;">​</a></h2><p>我最开始接触本地化运行大模型，是从 <code>ollama</code> 开始的。它的确非常方便，一条命令就能把模型跑起来：</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">ollama</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> run</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> qwen:7b</span></span></code></pre></div><p>执行这条命令后，最让我印象深刻的是下载过程。一个 <code>qwen:7b</code> 模型，下载的文件体积动辄 7GB、14GB。</p><p><img src="https://stack-mcell.tos-cn-shanghai.volces.com/052.png" alt="052.png" loading="lazy"></p><p>这让我非常困惑。我去 GitHub 上看 Qwen 的<a href="https://github.com/QwenLM/Qwen" target="_blank" rel="noreferrer">官方仓库</a>，把整个项目克隆下来，所有 Python 代码加起来也不过几十 MB。</p><p><strong>7GB 的庞然大物，和几十 MB 的代码，这两者是什么关系？</strong></p><p>一开始我以为，它是不是像传统软件一样，附带了一个巨大的数据库？运行的时候，代码从这个数据库里检索信息？但这个猜测很快被我自己推翻了，我逐渐了解到 LLM 的回答是生成式的，而不是检索式的。它可以创造出全新的、数据库里根本不存在的句子。</p><p>很长一段时间，我都错误地认为那些代码本身通过某种我无法理解的复杂算法实现了“智能”。直到最近，我才恍然大悟：<strong>我一直忽略了模型真正的核心——参数。</strong></p><h2 id="被下载的不是-数据-而是-大脑" tabindex="-1">被下载的不是“数据”，而是“大脑” <a class="header-anchor" href="#被下载的不是-数据-而是-大脑" aria-label="Permalink to &quot;被下载的不是“数据”，而是“大脑”&quot;">​</a></h2><p>现在我可以回答我自己的问题了。</p><p><img src="https://stack-mcell.tos-cn-shanghai.volces.com/054.png" alt="054.png" loading="lazy"></p><p>GitHub 上的代码，是 LLM 的 <strong>“骨架”<strong>或者说</strong>“引擎”</strong> 。它定义了一个叫做“Transformer”的神经网络结构，并规定了数据（也就是我们的文字）如何在其中流动和计算。它本身是“空”的，没有任何知识。</p><p>而我们通过 <code>ollama</code> 下载的那个 7GB 的文件，才是 LLM 的 <strong>“大脑”<strong>和</strong>“灵魂”</strong> 。它的官方名字叫做 <strong>模型参数（Parameters）<strong>或</strong>模型权重（Weights）</strong>。</p><p>这个文件里存储的是数十亿（7B 就是 70 亿）个经过训练优化的浮点数。这些数字，就是模型从数万亿单词的语料库中学习到的所有知识的浓缩和结晶。</p><p>这个学习过程被称为 <strong>“训练”</strong> 。你可以把它想象成一个极其复杂的拟合过程。模型看到“天空是”这句话，它被要求预测下一个词。它一开始会瞎猜，比如猜“绿色的”。然后我们告诉它，标准答案是“蓝色的”。它就会微调内部那 70 亿个参数，让自己下一次遇到类似情况时，猜出“蓝色的”概率高一点点。</p><p>这个过程重复数万亿次之后，模型内部的参数就形成了一种极其微妙的平衡。它不再是简单地记忆，而是学会了语法、逻辑、事实，甚至是某种程度上的“推理”能力。</p><p>所以，当我们输入“早上好”时，整个流程是这样的：</p><ol><li><strong>输入处理</strong>：推理代码（骨架）首先将“早上好”这三个汉字，通过一个叫做 Tokenizer 的工具，转换成一串数字 ID（比如 <code>[234, 567, 890]</code>）。</li><li><strong>矩阵运算</strong>：这串数字被输入到模型网络中，与那 7GB 文件里的 70 亿个参数进行一系列大规模的矩阵乘法运算。</li><li><strong>概率输出</strong>：运算的结果，是模型预测出的词汇表里每一个词在当前位置出现的概率。比如，“！”的概率可能是 30%，“今”的概率可能是 20%，“你”的概率可能是 15%……</li><li><strong>文本生成</strong>：代码根据这些概率，选择一个词（通常是概率最高的那个）作为输出，然后把这个新生成的词再作为新的输入，重复上述过程，直到生成完整的句子。</li></ol><p><img src="https://stack-mcell.tos-cn-shanghai.volces.com/055.png" alt="055.png" loading="lazy"></p><p>整个过程，没有一行 <code>if-else</code> 来判断用户意图，全都是冰冷的、确定性的数学计算。所谓的“智能”和“理解”，就蕴含在那 70 亿个参数构成的复杂函数之中。</p><h2 id="从规则到概率的飞跃" tabindex="-1">从规则到概率的飞跃 <a class="header-anchor" href="#从规则到概率的飞跃" aria-label="Permalink to &quot;从规则到概率的飞跃&quot;">​</a></h2><p>想通了这一点，我有一种豁然开朗的感觉。</p><p>我们正处在一个范式转换的时代。传统的编程思维是<strong>基于规则的、确定性的</strong>。我们告诉计算机每一步该做什么。而 LLM 的思维是<strong>基于概率的、涌现性的</strong>。我们构建一个能够学习的框架，然后用海量数据“喂养”它，让“智能”从中自然“涌现”出来。</p><p>这或许就是为什么 LLM 能处理如此复杂和模糊的人类语言的原因。因为语言本身，在很多时候就不是一个严格的逻辑系统，而是一个充满了习惯、文化和上下文的概率系统。</p><p>所以，LLM 不是一个装满了数据的“超级数据库”，它是一个学会了语言规律的“概率计算引擎”。它的代码是骨架，而巨大的参数文件，才是它智慧的真正载体。</p><p><strong>参考链接：</strong></p><ol><li><a href="http://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noreferrer">The Illustrated Transformer</a> (一篇非常经典的图解 Transformer 的文章)</li><li><a href="https://ollama.com/" target="_blank" rel="noreferrer">Ollama 官方网站</a></li><li><a href="https://github.com/QwenLM/Qwen" target="_blank" rel="noreferrer">Qwen (通义千问) GitHub 仓库</a></li></ol><p>(完)</p>`,41)]))}const c=a(e,[["render",l]]);export{d as __pageData,c as default};
