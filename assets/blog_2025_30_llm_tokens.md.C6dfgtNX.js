import{_ as n,c as i,o as a,a9 as t}from"./chunks/framework.DsVI9alt.js";const c=JSON.parse('{"title":"LLM 扫盲：什么是 Tokens？","description":"重新梳理 LLM 的 Token 概念，解析 GPT-5.1 的多语言编码优化、计费与上下文窗口限制，并给出本地用 tiktoken 预估 Token 的实践方案。","frontmatter":{"title":"LLM 扫盲：什么是 Tokens？","description":"重新梳理 LLM 的 Token 概念，解析 GPT-5.1 的多语言编码优化、计费与上下文窗口限制，并给出本地用 tiktoken 预估 Token 的实践方案。","author":"mcell","tags":["LLM","Tokens","Tokenization","上下文窗口","Prompt工程","计费","GPT-5.1","tiktoken","API成本","RAG"],"keywords":["Token是什么","LLM Token计费","GPT-5.1编码","中文Token效率","tiktoken计算","Token上下文窗口","LLM成本控制","Prompt长度管理","子词分词","RAG优化"],"head":[["link",{"rel":"canonical","href":"https://stack.mcell.topblog/2025/30_llm_tokens"}],["meta",{"property":"og:url","content":"https://stack.mcell.topblog/2025/30_llm_tokens"}]]},"headers":[],"relativePath":"blog/2025/30_llm_tokens.md","filePath":"blog/2025/30_llm_tokens.md","lastUpdated":1764697269000}'),e={name:"blog/2025/30_llm_tokens.md"};function o(l,s,k,p,h,r){return a(),i("div",null,s[0]||(s[0]=[t(`<p><img src="https://stack-mcell.tos-cn-shanghai.volces.com/072.jpg" alt="072" loading="lazy"></p><h1 id="llm-扫盲-什么是-tokens" tabindex="-1">LLM 扫盲：什么是 Tokens？ <a class="header-anchor" href="#llm-扫盲-什么是-tokens" aria-label="Permalink to &quot;LLM 扫盲：什么是 Tokens？&quot;">​</a></h1><p>GPT 5.1 发布已经有一段时间了，LLM（大语言模型）的能力边界再一次被拓宽。对于应用开发者而言，虽然模型越来越智能，但 API 的计费逻辑和底层限制依然没有变：<strong>Token</strong> 始终是那个核心计量单位。</p><p>很多人对 Token 有误解，认为它等同于字符（Character）或单词（Word）。这种误解往往导致两个问题：一是预估 API 成本时出现较大偏差，二是无法精确控制 Prompt 的上下文长度，导致模型“失忆”。</p><p>今天，我们再来系统地梳理一下 Token 的概念。</p><h2 id="机器如何阅读文本" tabindex="-1">机器如何阅读文本？ <a class="header-anchor" href="#机器如何阅读文本" aria-label="Permalink to &quot;机器如何阅读文本？&quot;">​</a></h2><p>计算机只能处理数字，不能直接处理文本。因此，当我们向 LLM 发送一段话时，必须经历一个转码过程。</p><ol><li><strong>输入文本</strong>：人类语言。</li><li><strong>Tokenization（分词）</strong>：将文本切分成一个个具备语义的最小单位（Token），并转换为数字 ID。</li><li><strong>模型计算</strong>：模型对这些数字 ID 进行预测和计算。</li></ol><p><strong>Token</strong> 就是这个中间层的最小单位。</p><p>为什么不直接用“汉字”或“单词”做单位？</p><ul><li><strong>字符粒度太细</strong>：如果用字符（如 <code>a</code>, <code>b</code>, <code>c</code>），语义太稀疏，模型计算量会呈指数级上升。</li><li><strong>单词粒度太粗</strong>：人类语言词汇量太大，且不断有新词产生，这会导致模型的词表（Vocabulary）过于庞大。</li></ul><p>因此，LLM 采用的是 <strong>Sub-word（子词）</strong> 方案：常用的词是一个 Token，不常用的词拆分成多个 Token。</p><h2 id="token-的切分原理" tabindex="-1">Token 的切分原理 <a class="header-anchor" href="#token-的切分原理" aria-label="Permalink to &quot;Token 的切分原理&quot;">​</a></h2><p>Token 的切分规则并非一成不变，不同模型使用的编码器（Encoding）不同，结果也不同。</p><h3 id="英文与中文的差异" tabindex="-1">英文与中文的差异 <a class="header-anchor" href="#英文与中文的差异" aria-label="Permalink to &quot;英文与中文的差异&quot;">​</a></h3><ul><li><strong>英文</strong>：通常一个单词是一个 Token，但复杂的词会被拆分。例如 <code>smart</code> 是一个，但合成词或生僻词会被拆解。</li><li><strong>中文</strong>：在 GPT-3 时，中文非常“吃亏”，一个汉字往往需要 2-3 个 Token。</li></ul><p>到了现在的 <strong>GPT-5.1</strong>，Token 编码（如 <code>o200k_base</code> 或更新的编码集）对多语言进行了深度优化。 目前，绝大多数常用汉字，<strong>1 个汉字 = 1 个 Token</strong>。只有极少数生僻字或复杂的古文，才会被拆解。</p><p>这意味着，同样的预算，现在能处理的中文内容比两三年前多了将近一倍。</p><h3 id="代码演示-node-js" tabindex="-1">代码演示（Node.js） <a class="header-anchor" href="#代码演示-node-js" aria-label="Permalink to &quot;代码演示（Node.js）&quot;">​</a></h3><p>光说概念比较抽象，我们直接看代码。</p><p>在 Web 开发或 Node.js 环境中，我们通常使用 npm 包 <code>@dqbd/tiktoken</code> 来在本地计算 Token 数，这比每次调用 API 估算要快得多，也更省钱。</p><p>安装：</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">npm</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> install</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> @dqbd/tiktoken</span></span></code></pre></div><p>代码示例：</p><div class="language-javascript vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">javascript</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> { encoding_for_model } </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;@dqbd/tiktoken&quot;</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">// 获取 GPT-5.1</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">const</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> enc</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> encoding_for_model</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;GPT-5.1&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">const</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> text</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;AI技术&quot;</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">// 将文本转换为 Token ID 数组</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">const</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> tokens</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> enc.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">encode</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(text)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">console.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">log</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(tokens)</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">// 输出可能是: Uint32Array(2) [ 12345, 67890 ]</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">// 解释：&#39;AI&#39; 是一个 Token，&#39;技术&#39; 作为一个常用词可能被编码为一个 Token，或者两个汉字各一个。</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">console.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">log</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Token Count:&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, tokens.</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">length</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">// 记得释放内存</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">enc.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">free</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">()</span></span></code></pre></div><p><img src="https://stack-mcell.tos-cn-shanghai.volces.com/071.png" alt="71" loading="lazy"></p><p>通过这种方式，你可以在发送请求前，精确地知道这段文本会消耗多少 Token。</p><h2 id="token-的实际影响" tabindex="-1">Token 的实际影响 <a class="header-anchor" href="#token-的实际影响" aria-label="Permalink to &quot;Token 的实际影响&quot;">​</a></h2><p>理解 Token 主要为了解决两个现实问题。</p><h3 id="计费-cost" tabindex="-1">计费（Cost） <a class="header-anchor" href="#计费-cost" aria-label="Permalink to &quot;计费（Cost）&quot;">​</a></h3><p>API 计费公式通常是：<strong>（Input Tokens + Output Tokens）× 单价</strong>。</p><p>值得注意的是，随着模型迭代（如 GPT-5.1），推理成本虽然在下降，但 Output（生成内容）的价格通常依然高于 Input（输入内容）。</p><ul><li><strong>Input</strong>：你发给模型的 Prompt。</li><li><strong>Output</strong>：模型生成的回答。</li></ul><p>如果你的业务场景是“读长文、写摘要”，成本相对可控；如果是“读短句、写长文”，成本会显著增加。</p><h3 id="上下文窗口-context-window" tabindex="-1">上下文窗口（Context Window） <a class="header-anchor" href="#上下文窗口-context-window" aria-label="Permalink to &quot;上下文窗口（Context Window）&quot;">​</a></h3><p>这是 Token 最关键的物理限制。</p><p>虽然 GPT-5.1 的上下文窗口已经非常大（通常在 128k 甚至 200k tokens 以上），但它依然不是无限的。</p><ul><li><strong>早期模型</strong>：GPT-3.5 只有 4k context（约 3000 汉字），稍微聊几句就得“遗忘”前面的对话。</li><li><strong>当前模型</strong>：128k context 意味着你可以一次性把几本长篇小说塞给模型。</li></ul><p>但是，<strong>“能塞进去”不代表“效果好”</strong>。虽然 Token 容量变大了，但输入的内容越多，模型对中间信息的“注意力”（Attention）可能会被稀释。因此，开发者依然需要利用 RAG（检索增强生成）等技术，精简输入给模型的 Token，这不仅是为了省钱，更是为了提高回答的准确率。</p><h2 id="四、-总结" tabindex="-1">四、 总结 <a class="header-anchor" href="#四、-总结" aria-label="Permalink to &quot;四、 总结&quot;">​</a></h2><ol><li><strong>Token 是计费和计算的单位</strong>：它介于字符和单词之间。</li><li><strong>中文效率已大幅提升</strong>：在 GPT-5.1 时代，中英文的 Token 效率差距已大大缩小，基本可以按 1 字 = 1 Token 估算。</li><li><strong>开发者应当在本地计算</strong>：使用 <code>@dqbd/tiktoken</code> 等库在本地预计算 Token，是控制成本和上下文管理的最佳实践。</li></ol><p>理解 Token，是开发 LLM 应用的第一步，也是从“用户”进阶为“开发者”的必修课。</p><p>（完）</p>`,43)]))}const g=n(e,[["render",o]]);export{c as __pageData,g as default};
